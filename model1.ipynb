{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import xlogy\n",
    "\n",
    "# 自带特征重要性\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SMOTEN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler, NearMiss, EditedNearestNeighbours, AllKNN, CondensedNearestNeighbour, OneSidedSelection, TomekLinks, RepeatedEditedNearestNeighbours, NeighbourhoodCleaningRule, InstanceHardnessThreshold\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(path):\n",
    "    df = pd.read_csv(path, encoding='gbk', dtype=float)\n",
    "    # 哑变量处理\n",
    "    raw_col = ['a1b1_a', 'a1e1', 'a1_1', 'a2', 'a3', 'a4a', 'a5', 'b1', 'b8b', 'c1_0', 'c3a',\n",
    "                'd1_1', 'd1_2', 'd1_3', 'd1_4', 'd1_5', 'd1_6', 'd1_7', 'd1_8', 'd1_9', 'd1_10', 'd1_11',\n",
    "                'd4b2_1', 'd4b2_2', 'd4b2_3', 'd4b2_4', 'd4b2_5', 'd4b2_6', 'd4b2_7', 'd4b2_8', 'd4b2_9', 'd4b2_10', 'd4b2_11',\n",
    "                'd4b3_1', 'd4b3_2', 'd4b3_3', 'd4b3_4', 'd4b3_5', 'd4b3_6', 'e1a_1', 'e1a_2', 'e1a_3', 'e1a_4', 'e1a_5', 'e1a_6',\n",
    "                'f4b2_1', 'f4b2_2', 'f4b2_3',   'f4b3_1', 'f4b3_2', 'f4b3_3','g4a', 'g4b',\n",
    "                'g5_1', 'g5_2', 'g5_3', 'g5_4', 'g5_5', 'g5_6', 'g5_7', 'g5_8', 'g5_9', 'g5_10', 'g5_12', 'g5_13', 'g5_14', 'g5_11', 'g5_15',\n",
    "                'g7_1', 'g7_2', 'g7_3', 'g7_4', 'g7_5', 'g7_6', 'g7_7', 'g7_8', 'g7_9', 'g7_10', 'g7_11', 'g7_12', 'g7_13', 'g7_14', 'g7_15', 'g7_16', 'g7_17', 'g7_18', 'g7_19', 'g7_20', 'g7_21',\n",
    "                'h1a_1', 'h1a_2', 'h1a_3', 'h1a_4', 'h1a_5', 'h1a_6', 'h1c_a',\n",
    "                'i1_1', 'i1_2', 'i1_3', 'i1_4', 'i1_5', 'i1_6', 'i1_7', 'i1_8', 'i1_9', 'i1_10', 'i1_12', 'i1_13', 'i1_11',\n",
    "                'i6', 'i7', 'i8_1', 'i8_2', 'i8_3', 'i8_4', 'i8_5', 'i8_6', 'i8_7', 'i8_8',\n",
    "                'i9_1', 'i9_2', 'i9_3', 'i9_4', 'i9_5', 'i9_6', 'i9_7', 'i9_8', 'i9_9', 'i9_10', 'i9_11', 'i9_12', 'i9_13', 'i9_14',\n",
    "                'cx2', 'dzx']\n",
    "    dummies_col = [df]\n",
    "    for i in range(len(raw_col)):\n",
    "        dummies_col.append(pd.get_dummies(df[raw_col[i]], prefix=raw_col[i]))\n",
    "    \n",
    "    df = pd.concat(dummies_col, axis=1)\n",
    "    df.drop(columns=raw_col, axis=1, inplace=True)\n",
    "    df.drop(columns = ['ID'], inplace=True)\n",
    "\n",
    "    y = df['tag']\n",
    "    df = df.drop(columns = ['tag'])\n",
    "    x = df\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    y_train = y_train.astype('int')\n",
    "    y_test = y_test.astype('int')\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据采样方法\n",
    "sampling_methods = {\n",
    "    'None': None,\n",
    "    # -- 上采样方法 --\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=42),\n",
    "    'SMOTE': SMOTE(),\n",
    "   # 'ADASYN': ADASYN(),\n",
    "   # 'BorderlineSMOTE': BorderlineSMOTE(),\n",
    "   # 'SMOTEN': SMOTEN(),\n",
    "    # -- 下采样方法 --\n",
    "    'ClusterCentroids': ClusterCentroids(random_state=42),\n",
    "    #'TomekLinks': TomekLinks(*, sampling_strategy='auto', n_jobs=None),\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42),\n",
    "   # 'NearMiss-1': NearMiss(version=1),\n",
    "   # 'NearMiss-2': NearMiss(version=2),\n",
    "   # 'NearMiss-3': NearMiss(version=3),\n",
    "    #'EditedNearestNeighbours': EditedNearestNeighbours(),\n",
    "    #'AllKNN': AllKNN(),\n",
    "   # 'CondensedNearestNeighbour': CondensedNearestNeighbour(random_state=42),\n",
    "   # 'OneSidedSelection': OneSidedSelection(random_state=42),\n",
    "   # 'RepeatedEditedNearestNeighbours': RepeatedEditedNearestNeighbours(),\n",
    "   # 'NeighbourhoodCleaningRule': NeighbourhoodCleaningRule(),\n",
    "   # 'InstanceHardnessThreshold': InstanceHardnessThreshold(random_state=42, estimator=LogisticRegression(solver='lbfgs', multi_class='auto')),\n",
    "  #  # -- 上采样和下采样混合方法 --\n",
    "   # 'SMOTEENN': SMOTEENN(random_state=42),\n",
    "    'SMOTETomek': SMOTETomek(random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaCast\n",
    "class AdaCostClassifier(AdaBoostClassifier):\n",
    "    \n",
    "    def _boost_real(self, iboost, X, y, sample_weight, random_state):\n",
    "        '''\n",
    "        权重更新的公式在这里\n",
    "        '''\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "\n",
    "        if iboost == 0:\n",
    "            self.classes_ = getattr(estimator, 'classes_', None) # 获取estimator的classes_属性值\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)\n",
    "\n",
    "        # 分类不正确的实例\n",
    "        incorrect = y_predict != y\n",
    "\n",
    "        # 误差分数\n",
    "        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "        # 如果分类器完美，那么就停止\n",
    "        if estimator_error <= 0:\n",
    "            return sample_weight, 1.0, 0.0\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_\n",
    "        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])\n",
    "        y_coding = y_codes.take(classes == y[:, np.newaxis])\n",
    "\n",
    "        proba = y_predict_proba  # 别名\n",
    "        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n",
    "\n",
    "        estimator_weight = (\n",
    "            -1.0\n",
    "            * self.learning_rate\n",
    "            * ((n_classes - 1.0) / n_classes)\n",
    "            * xlogy(y_coding, y_predict_proba).sum(axis=1)\n",
    "        )\n",
    "\n",
    "        # 在此处更新，增加代价敏感系数\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Only boost positive weights\n",
    "            sample_weight *= np.exp(\n",
    "                estimator_weight * ((sample_weight > 0) | (estimator_weight < 0)) * self._beta(y, y_predict)\n",
    "            )\n",
    "\n",
    "        return sample_weight, 1.0, estimator_error\n",
    "\n",
    "    def _beta(self, y, y_hat):\n",
    "        '''\n",
    "        代价调整函数\n",
    "        '''\n",
    "        res = []\n",
    "        for i in zip(y, y_hat):\n",
    "            if i[0] == i[1]:\n",
    "                res.append(1) # 正确分类，系数保持不变\n",
    "            elif i[0] == 1 and i[1] == -1:\n",
    "                res.append(1) # 将正类（好人）判断为负类（坏人）代价更大，系数增大\n",
    "            elif i[0] == -1 and i[1] == 1:\n",
    "                res.append(1) # 将负类（坏人）判断为正类（好人）代价更大，系数增大\n",
    "            else:\n",
    "                print(i[0], i[1])\n",
    "\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 使用LogisticRegression类构建Logistic回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def LogisticR(X_train, y_train, X_test, y_test):\n",
    "    lr_model = LogisticRegression(penalty='l2', solver='liblinear',random_state=0)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    return y_pred\n",
    "# AutoGluon\n",
    "# AutoGluon\n",
    "def auto_gluon(X_train, y_train, X_test, y_test):\n",
    "    # 分别合并训练数据和标签、测试数据和标签\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    train_data = TabularDataset(train_data)\n",
    "    test_data = TabularDataset(test_data)\n",
    "\n",
    "    # 训练\n",
    "    predictor = TabularPredictor(label='tag', path='ag_models').fit(train_data)\n",
    "    \n",
    "    # 预测\n",
    "    test_data_nolab = test_data.drop(columns = ['tag'])\n",
    "    predictor = TabularPredictor.load('ag_models')\n",
    "    y_pred = predictor.predict(test_data_nolab)\n",
    "    return y_pred\n",
    "\n",
    "# BaggingDT\n",
    "def BaggingDT(X_train, y_train, X_test, y_test):\n",
    "    bc = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=0)\n",
    "    bc.fit(X_train, y_train)\n",
    "    y_pred = bc.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# BalancedBaggingDT\n",
    "def BalancedBaggingDT(X_train, y_train, X_test, y_test):\n",
    "    bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                    n_estimators=100,\n",
    "                                    sampling_strategy='auto',\n",
    "                                    replacement=False,\n",
    "                                    random_state=42)\n",
    "    bbc.fit(X_train, y_train)\n",
    "    y_pred = bbc.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# BalancedRF\n",
    "def BalancedRF(X_train, y_train, X_test, y_test):\n",
    "    brf = BalancedRandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    brf.fit(X_train, y_train)\n",
    "    y_pred = brf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# RUSBoost\n",
    "def RUSBoost(X_train, y_train, X_test, y_test):\n",
    "    rusboost = RUSBoostClassifier(n_estimators=200, algorithm='SAMME.R',\n",
    "                                random_state=0)\n",
    "    rusboost.fit(X_train, y_train)\n",
    "    y_pred = rusboost.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# EasyEnsemble\n",
    "def EasyEnsemble(X_train, y_train, X_test, y_test):\n",
    "    eec = EasyEnsembleClassifier(random_state=0)\n",
    "    eec.fit(X_train, y_train)\n",
    "    y_pred = eec.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "# AdaCast\n",
    "def AdaCast(X_train, y_train, X_test, y_test):\n",
    "    acc = AdaCostClassifier(n_estimators=100)\n",
    "    acc.fit(X_train, y_train)\n",
    "    y_pred = acc.predict(X_test)\n",
    "    # y_pred -1变为1，1变为0\n",
    "    y_pred = np.where(y_pred == -1, 1, 0)\n",
    "    return y_pred\n",
    "\n",
    "classifiers = {\n",
    "    #'AutoGluon': auto_gluon,\n",
    "   # 'BaggingDT': BaggingDT,\n",
    "   # 'BalancedBaggingDT': BalancedBaggingDT,\n",
    "   # 'BalancedRF': BalancedRF,\n",
    "    #'RUSBoost': RUSBoost,\n",
    "    #'EasyEnsemble': EasyEnsemble,\n",
    "   # 'AdaCast': AdaCast,\n",
    "    'LogisticR': LogisticR\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(y_pred, y_test):\n",
    "    print('Accuracy: %lf' % accuracy_score(y_test, y_pred))\n",
    "    print('Balanced Accuracy: %lf' % balanced_accuracy_score(y_test, y_pred))\n",
    "    print('Precision: %lf' % precision_score(y_test, y_pred))\n",
    "    print('Recall: %lf' % recall_score(y_test, y_pred))\n",
    "    print('F1: %lf' % f1_score(y_test, y_pred))\n",
    "    print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "def result_to_csv_item(y_pred, y_test):\n",
    "    item1 = accuracy_score(y_test, y_pred)\n",
    "    item2 = balanced_accuracy_score(y_test, y_pred)\n",
    "    item3 = precision_score(y_test, y_pred)\n",
    "    item4 = recall_score(y_test, y_pred)\n",
    "    item5 = f1_score(y_test, y_pred)\n",
    "    item6 = confusion_matrix(y_test, y_pred)\n",
    "    return f'{item1},{item2},{item3},{item4},{item5},{item6[0][0]},{item6[0][1]},{item6[1][0]},{item6[1][1]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(y_test, y_pred, suffix='none'):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=[0, 1]).plot(cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'cms/confusion_matrix_{suffix}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_sampling(classifier, method, X_train, y_train, X_test, y_test):\n",
    "    # 采样\n",
    "    if method:\n",
    "        X_train, y_train = method.fit_resample(X_train, y_train)\n",
    "    # 采样后数据集大小\n",
    "    print('采样后训练集维数为：', X_train.shape, '\\n', '样本个数为：', y_train.shape)\n",
    "    # 训练和评估\n",
    "    y_pred = classifier(X_train, y_train, X_test, y_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集维数为： (3101, 465) \t 样本个数为： (3101,)\n",
      "测试集维数为： (776, 465) \t 样本个数为： (776,)\n",
      "Classifier: LogisticR \tSampling method: None\n",
      "采样后训练集维数为： (3101, 465) \n",
      " 样本个数为： (3101,)\n",
      "------------------------------------------\n",
      "Classifier: LogisticR \tSampling method: RandomOverSampler\n",
      "采样后训练集维数为： (3932, 465) \n",
      " 样本个数为： (3932,)\n",
      "------------------------------------------\n",
      "Classifier: LogisticR \tSampling method: SMOTE\n",
      "采样后训练集维数为： (3932, 465) \n",
      " 样本个数为： (3932,)\n",
      "------------------------------------------\n",
      "Classifier: LogisticR \tSampling method: ClusterCentroids\n",
      "采样后训练集维数为： (2270, 465) \n",
      " 样本个数为： (2270,)\n",
      "------------------------------------------\n",
      "Classifier: LogisticR \tSampling method: RandomUnderSampler\n",
      "采样后训练集维数为： (2270, 465) \n",
      " 样本个数为： (2270,)\n",
      "------------------------------------------\n",
      "Classifier: LogisticR \tSampling method: SMOTETomek\n",
      "采样后训练集维数为： (3390, 465) \n",
      " 样本个数为： (3390,)\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.system(\"export OMP_NUM_THREADS=8\")\n",
    "# 数据预处理\n",
    "X_train, X_test, y_train, y_test = data_preprocess('数据集1.csv')\n",
    "print('训练集维数为：', X_train.shape, '\\t', '样本个数为：', y_train.shape)\n",
    "print('测试集维数为：', X_test.shape, '\\t', '样本个数为：', y_test.shape)\n",
    "\n",
    "# 保存模型结果\n",
    "f_res = open('result.csv', 'a')\n",
    "f_res.write('Model,Sampler,Accuracy,Balanced Accuracy,Precision,Recall,F1-Score,A,B,C,D\\n')\n",
    "# 选择模型\n",
    "for classifier in classifiers:\n",
    "    #if classifier != 'AdaCast':\n",
    "       # continue\n",
    "    if classifier == 'AdaCast':\n",
    "        # AdaCast需要将标签转换为-1和1\n",
    "        y_train = np.where(y_train == 0, 1, -1)\n",
    "    \n",
    "    # 选择采样方法\n",
    "    for method in sampling_methods:\n",
    "\n",
    "       # if method != 'ClusterCentroids':\n",
    "            #continue\n",
    "\n",
    "        print(\"Classifier:\", classifier, '\\tSampling method:', method)\n",
    "        # 训练\n",
    "        y_pred = classify_and_sampling(classifiers[classifier], sampling_methods[method], X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # 指标\n",
    "       # print_result(y_pred, y_test)\n",
    "       # draw_confusion_matrix(y_test, y_pred, f'{classifier}_{method}')\n",
    "\n",
    "        f_res.write(f'{classifier},{method},{result_to_csv_item(y_pred, y_test)}\\n')    \n",
    "        print('------------------------------------------')\n",
    "        f_res.flush()\n",
    "\n",
    "f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl",
   "language": "python",
   "name": "mydemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc5d045b2d40e987b6a37800b8a87d0c868019fa9594e4647f7944268a0724c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
